<?xml version="1.0"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<configuration>
  <!-- Resource Manager MapR HA Configs -->
  <property>
    <name>yarn.resourcemanager.ha.custom-ha-enabled</name>
    <value>true</value>
    <description>MapR Zookeeper based RM Reconnect Enabled. If this is true, set the failover proxy to be the class MapRZKBasedRMFailoverProxyProvider</description>
  </property>
  <property>
    <name>yarn.client.failover-proxy-provider</name>
    <value>org.apache.hadoop.yarn.client.MapRZKBasedRMFailoverProxyProvider</value>
    <description>Zookeeper based reconnect proxy provider. Should be set if and only if mapr-ha-enabled property is true.</description>
  </property>
  <property>
    <name>yarn.resourcemanager.recovery.enabled</name>
    <value>true</value>
    <description>RM Recovery Enabled</description>
  </property>
  <property>
   <name>yarn.resourcemanager.ha.custom-ha-rmaddressfinder</name>
   <value>org.apache.hadoop.yarn.client.MapRZKBasedRMAddressFinder</value>
  </property>

  <property>
    <name>yarn.acl.enable</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.admin.acl</name>
    <value> </value>
  </property>

  <!-- :::CAUTION::: DO NOT EDIT ANYTHING ON OR ABOVE THIS LINE -->
    <!-- fix for Oozie when user different than mapr -->
    <property>
        <name>yarn.resourcemanager.principal</name>
        <value>{{ mapr_user }}</value>
    </property>
    <property>
        <name>yarn.nodemanager.container-executor.class</name>
        <value>org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor</value>
    </property>
    <property>
        <name>yarn.nodemanager.linux-container-executor.group</name>
        <value>{{ mapr_group }}</value>
    </property>
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>{{ "true" if yarn_log_aggregation else "false" }}</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>{{ yarn_max_container_allocation_in_mb }}</value>
        <description>The maximum allocation for every container request at the RM, in MBs. Memory requests higher than this will throw a InvalidResourceRequestException.</description>
    </property>
    <property>
        <name>yarn.resourcemanager.am.max-attempts</name>
        <value>4</value>
        <description>The maximum number of application attempts</description>
    </property>
    {% if yarn_local_dir_to_maprfs %}
    <property>
        <name>yarn.nodemanager.local-dirs</name>
        <value>/mapr/{{ cluster_name }}/var/mapr/local/{{ ansible_fqdn }}/nm-local-dir</value>
    </property>
    {% endif %}
    {% if yarn_spark_shuffle %}
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle,mapr_direct_shuffle,spark_shuffle</value>
        <description>shuffle service that needs to be set for Map Reduce to run</description>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>
        <value>org.apache.spark.network.yarn.YarnShuffleService</value>
    </property>
    <property>
        <name>spark.shuffle.service.port</name>
        <value>7337</value>
        <description>Port on which the external shuffle service will run.</description>
    </property>
    {% if spark_sasl_encryption %}
    <property>
        <name>spark.authenticate</name>
        <value>true</value>
        <description>Whether Spark authenticates its internal connections.</description>
    </property>
    {% endif %}
    <property>
        <name>spark.yarn.shuffle.stopOnFailure</name>
        <value>false</value>
        <description>Whether to stop the NodeManager when there's a failure in the Spark Shuffle Service's initialization.
            This prevents application failures caused by running containers on NodeManagers where the Spark Shuffle Service is not running.
        </description>
    </property>
    {% endif %}
    {% if mapr_security == 'kerberos' %}
    <property>
        <name>yarn.nodemanager.keytab</name>
        <value>/opt/mapr/conf/mapr.keytab</value>    <!-- path to the YARN keytab -->
    </property>
    <property>
        <name>yarn.nodemanager.principal</name>
        <value>{{ mapr_user }}/_HOST@{{ mapr_kerberos_realm }}</value>
    </property>
    <property>
        <name>yarn.resourcemanager.keytab</name>
        <value>/opt/mapr/conf/mapr.keytab</value>    <!-- path to the YARN keytab -->
    </property>
    <property>
        <name>yarn.resourcemanager.principal</name>
        <value>{{ mapr_user }}/_HOST@{{ mapr_kerberos_realm }}</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.spnego-keytab-file</name>
        <value>/opt/mapr/conf/mapr.keytab</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.spnego-principal</name>
        <value>HTTP/_HOST@{{ mapr_kerberos_realm }}</value>
    </property>
    <property>
        <name>yarn.nodemanager.webapp.spnego-keytab-file</name>
        <value>/opt/mapr/conf/mapr.keytab</value>
    </property>
    <property>
        <name>yarn.nodemanager.webapp.spnego-principal</name>
        <value>HTTP/_HOST@{{ mapr_kerberos_realm }}</value>
    </property>
    <property>
        <name>yarn.mapr.ticket.expiration</name>
        <value>3154000000000</value>
    </property>
    {% endif %}

</configuration>
